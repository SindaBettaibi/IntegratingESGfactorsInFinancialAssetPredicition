{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2030d7-0b46-4a86-8df7-c3112a8f1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from plotly import tools\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import init_notebook_mode, iplot, iplot_mpl\n",
    "stock_start_date = '2021-01-01'\n",
    "stock_end_date = '2023-12-31'\n",
    "df = yf.download('MSFT', start= stock_start_date, end=stock_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdacb8b-ce0d-445e-96f3-1a9949dfbab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a1a1e-964f-4e69-8420-ceec3a7ee519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the price for printing\n",
    "def formatPrice(n):\n",
    "    return (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -100, 100)))  # Clip values to prevent overflow\n",
    "\n",
    "def getState(data, t, window_size):\n",
    "    # Ensuring there is enough data to create a full state\n",
    "    d = t - window_size + 1\n",
    "    if d < 0:\n",
    "        # Padding the data if 'd' is negative\n",
    "        block = np.pad(data[:t + 1], ((abs(d), 0), (0, 0)), 'constant', constant_values=(0))\n",
    "    else:\n",
    "        block = data[d:t + 1]\n",
    "\n",
    "    res = [sigmoid(block[i + 1] - block[i]) for i in range(min(len(block) - 1, window_size - 1))]\n",
    "    # Padding the result to ensure it always has 'window_size - 1' elements\n",
    "    if len(res) < window_size - 1:\n",
    "        res = [0] * (window_size - 1 - len(res)) + res\n",
    "\n",
    "    # Ensure the output shape matches the expected input shape of the model\n",
    "    return np.array([res]).reshape(1, -1)\n",
    "\n",
    "def plot_behavior(data_input, states_buy, states_sell, profit):\n",
    "    # Assuming data_input is a 2D array with 6 features, where the 'Close' price is the 4th column (index 3)\n",
    "    #close_prices = data_input[:, 3]  # Extracting the 'Close' prices\n",
    "\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.plot(data_input, color='r', lw=2.)  # Plot the 'Close' price line\n",
    "\n",
    "    # Mark the buy and sell points\n",
    "    # markevery parameter expects a list of indices, here we directly use states_buy and states_sell\n",
    "    plt.plot(data_input, '^', markersize=10, color='m', label = 'Buying signal', markevery = states_buy)\n",
    "    plt.plot(data_input, 'v', markersize=10, color='k', label = 'Selling signal', markevery = states_sell)\n",
    "\n",
    "    plt.title('Total gains: %f' % profit)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865707a-225c-470b-9837-84e8ab8ddf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, window_size, model_name=\"\", is_eval=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = 3  # sit, buy, sell\n",
    "        self.memory = deque(maxlen=2000)  # Increased memory size\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "\n",
    "        self.gamma = 0.95  # Discount rate\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995  # More aggressive decay\n",
    "        self.learning_rate = 0.001  # Initial learning rate\n",
    "        self.learning_rate_decay = 0.999  # Learning rate decay\n",
    "        self.model = load_model(model_name) if is_eval else self._model()\n",
    "\n",
    "    def _model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(self.state_size,)))\n",
    "        model.add(Dense(units=64, activation=\"relu\"))\n",
    "        model.add(Dense(units=32, activation=\"relu\"))\n",
    "        model.add(Dense(units=16, activation=\"relu\"))  # Adjusted architecture\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        if not self.is_eval and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        options = self.model.predict(state)\n",
    "        return np.argmax(options[0])\n",
    "\n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        states = np.zeros((len(mini_batch), self.state_size))\n",
    "        next_states = np.zeros((len(mini_batch), self.state_size))\n",
    "\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(mini_batch):\n",
    "            if state.shape == (1, self.state_size) and next_state.shape == (1, self.state_size):\n",
    "                states[i] = state\n",
    "                next_states[i] = next_state\n",
    "            else:\n",
    "                continue  # Skip any invalid states\n",
    "\n",
    "        # Proceed with the calculation only if there are valid states\n",
    "        if np.any(states):\n",
    "            target_f = self.model.predict(states)\n",
    "            next_Q_values = self.model.predict(next_states)\n",
    "            for i, (state, action, reward, next_state, done) in enumerate(mini_batch):\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target = reward + self.gamma * np.amax(next_Q_values[i])\n",
    "                target_f[i][action] = target\n",
    "\n",
    "            # Fit the model\n",
    "            self.model.fit(states, target_f, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b738f30-bb6c-4e34-afdf-64e5494b77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code to run the training loop\n",
    "window_size = 1  # This should reflect the number of timesteps each state considers\n",
    "feature_count = 6  # Number of features per timestep\n",
    "state_size = window_size * feature_count\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = Agent(state_size=state_size, window_size=window_size)\n",
    "\n",
    "#In this step we feed the closing value of the stock price\n",
    "data = X_train\n",
    "l = len(data) - 1\n",
    "#\n",
    "batch_size = 32\n",
    "#An episode represents a complete pass over the data.\n",
    "episode_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e20f17-592c-4837-91df-4d1c3eaff978",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(episode_count + 1):\n",
    "    print(f\"Running episode {e}/{episode_count}\")\n",
    "    state = getState(data, 0, window_size)\n",
    "    total_profit = 0\n",
    "    agent.inventory = []\n",
    "    states_sell = []\n",
    "    states_buy = []\n",
    "\n",
    "    for t in range(l):\n",
    "        action = agent.act(state)  # Agent takes an action\n",
    "        next_state = getState(data, t + 1, window_size)  # Observe the next state\n",
    "        reward = 0\n",
    "\n",
    "        # Take action based on the current state\n",
    "        if action == 1:  # Buy\n",
    "            agent.inventory.append(data[t][3])\n",
    "            states_buy.append(t)\n",
    "            print(\"Buy: \" + formatPrice(data[t][3]))\n",
    "\n",
    "        elif action == 2 and len(agent.inventory) > 0:  # Sell\n",
    "            bought_price = agent.inventory.pop(0)\n",
    "            reward = max(data[t][3] - bought_price, 0)\n",
    "            total_profit += data[t][3] - bought_price\n",
    "            states_sell.append(t)\n",
    "            print(\"Sell: \" + formatPrice(data[t][3]) + \" | Profit: \" + formatPrice(data[t][3] - bought_price))\n",
    "\n",
    "        done = t == l - 1  # Check if we're at the end of the episode\n",
    "\n",
    "        # Store the transition in memory\n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform experience replay if the memory is sufficient\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.expReplay(batch_size)\n",
    "\n",
    "        # If done, print the total profit and plot the trades\n",
    "        if done:\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"Total Profit: \" + formatPrice(total_profit))\n",
    "            print(\"--------------------------------\")\n",
    "            plot_behavior(data[:, 3], states_buy, states_sell, total_profit)\n",
    "\n",
    "    # Save the model after each episode\n",
    "    agent.model.save(\"model_ep\" + str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
